import { createClient } from "@/lib/supabase/server";
import { NextRequest, NextResponse } from "next/server";
import { writeFile, mkdir } from "fs/promises";
import { join } from "path";
import { tmpdir } from "os";
import { cleanupOldChunks } from "@/lib/utils/cleanup-chunks";

export const maxDuration = 60;
export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

/**
 * POST /api/consultations/upload-chunk
 * 
 * Recebe chunks individuais de um arquivo de √°udio grande
 * Cada chunk √© salvo temporariamente at√© que todos sejam recebidos
 * 
 * IMPORTANTE: Vercel tem limite de 4.5MB por request
 * 
 * FormData:
 *   - chunk: Blob (max 4.5MB - limite Vercel)
 *   - sessionId: string (ID √∫nico da sess√£o de upload)
 *   - chunkIndex: number (√≠ndice deste chunk, come√ßando em 0)
 *   - totalChunks: number (total de chunks esperados)
 */
export async function POST(request: NextRequest) {
  try {
    const supabase = await createClient();

    // Verificar autentica√ß√£o
    const {
      data: { user },
    } = await supabase.auth.getUser();

    if (!user) {
      return NextResponse.json(
        { error: "N√£o autenticado" },
        { status: 401 }
      );
    }

    // Parsear FormData
    let formData: FormData;
    try {
      formData = await request.formData();
    } catch (formError: any) {
      console.error("‚ùå Erro ao parsear FormData do chunk:", formError);
      return NextResponse.json(
        { error: "Erro ao processar chunk" },
        { status: 400 }
      );
    }

    const chunkBlob = formData.get("chunk") as Blob;
    const sessionId = formData.get("sessionId") as string;
    const chunkIndex = parseInt(formData.get("chunkIndex") as string);
    const totalChunks = parseInt(formData.get("totalChunks") as string);

    // Validar par√¢metros
    if (!chunkBlob || !sessionId || isNaN(chunkIndex) || isNaN(totalChunks)) {
      return NextResponse.json(
        { error: "Par√¢metros inv√°lidos" },
        { status: 400 }
      );
    }

    // Validar tamanho do chunk (deve ser <= 4.5MB - limite da Vercel)
    const MAX_CHUNK_SIZE = 4.5 * 1024 * 1024; // 4.5MB - limite Vercel
    if (chunkBlob.size > MAX_CHUNK_SIZE) {
      return NextResponse.json(
        { error: `Chunk muito grande: ${(chunkBlob.size / 1024 / 1024).toFixed(2)}MB (max: 4.5MB - limite Vercel)` },
        { status: 400 }
      );
    }

    // Validar √≠ndice do chunk
    if (chunkIndex < 0 || chunkIndex >= totalChunks) {
      return NextResponse.json(
        { error: `√çndice de chunk inv√°lido: ${chunkIndex} (total: ${totalChunks})` },
        { status: 400 }
      );
    }

    console.log(
      `üì¶ Recebendo chunk ${chunkIndex + 1}/${totalChunks} ` +
      `(${(chunkBlob.size / 1024 / 1024).toFixed(2)}MB) - Session: ${sessionId.substring(0, 20)}...`
    );

    // Limpar chunks √≥rf√£os periodicamente (apenas no primeiro chunk para evitar overhead)
    if (chunkIndex === 0) {
      cleanupOldChunks().catch(err => {
        console.warn("‚ö†Ô∏è Erro na limpeza autom√°tica de chunks:", err);
        // N√£o √© cr√≠tico, continuar
      });
    }

    // Criar diret√≥rio para a sess√£o se n√£o existir
    const sessionDir = join(tmpdir(), 'audio-chunks', sessionId);
    await mkdir(sessionDir, { recursive: true });

    // Salvar chunk
    const chunkFileName = `chunk-${chunkIndex.toString().padStart(4, '0')}.bin`;
    const chunkPath = join(sessionDir, chunkFileName);
    const chunkBuffer = Buffer.from(await chunkBlob.arrayBuffer());

    await writeFile(chunkPath, chunkBuffer);

    console.log(`‚úÖ Chunk ${chunkIndex + 1}/${totalChunks} salvo: ${chunkFileName}`);

    // Calcular progresso
    const progress = Math.round(((chunkIndex + 1) / totalChunks) * 100);

    return NextResponse.json({
      success: true,
      chunkIndex,
      totalChunks,
      progress,
      message: `Chunk ${chunkIndex + 1}/${totalChunks} recebido`,
    });
  } catch (error: any) {
    console.error("‚ùå Erro ao processar chunk:", error);
    return NextResponse.json(
      { error: error.message || "Erro ao processar chunk" },
      { status: 500 }
    );
  }
}
